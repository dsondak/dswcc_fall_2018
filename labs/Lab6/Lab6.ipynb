{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6:  More on Convolutional Neural Networks\n",
    "\n",
    "## Saturday, November 17th 2018\n",
    "\n",
    "**David Sondak and Pavlos Protopapas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will cover the following topics:\n",
    "\n",
    "* [Training a Convnet from Scratch on a small dataset](#part0)\n",
    "    * [Using a small dataset](#part21)\n",
    "    * [The relevance of deep learning for small-data problems](#part22)\n",
    "    * [Data setup](#part23)\n",
    "    * [Building your network](#part24)\n",
    "    * [Data preprocessing](#part25)\n",
    "* [Data Augmentation](#part1) \n",
    "* [Comparing Two Networks](#part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part0'></a>\n",
    "## Last Time - Training a Convnet from Scratch on a small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='part21'></a>\n",
    "### Part 2.1 - Using a small dataset\n",
    "\n",
    "Having to train an image-classification model using very little data is a common situation, which you’ll likely encounter in practice if you ever do computer vision in a professional context. A \"few\" samples can mean anywhere from a few hundred to a few tens of thousands of images. As a practical example, we’ll focus on classifying images as dogs or cats in a dataset containing 4,000 pictures of cats and dogs (2,000 cats, 2,000 dogs). We’ll use 2,000 pictures for training: 1,000 for validation, and 1,000 for testing.\n",
    "\n",
    "In this section, we’ll review one basic strategy to tackle this problem: training a new model from scratch using what little data you have. You’ll start by naively training a small convnet on the 2,000 training samples, without any regularization, to set a baseline for what can be achieved. This will get you to a classification accuracy of 71%. At that point, the main issue will be overfitting. Then we’ll introduce *data augmentation*, a powerful technique for mitigating overfitting in computer vision. By using data augmentation, you’ll improve the network to reach an accuracy of 82%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part22'></a>\n",
    "### Part 2.2 - The relevance of deep learning for small-data problems\n",
    "\n",
    "You’ll sometimes hear that deep learning only works when lots of data is available. This is valid in part: one fundamental characteristic of deep learning is that it can find interesting features in the training data on its own, without any need for manual feature engineering, and this can only be achieved when lots of training examples are available. This is especially true for problems where the input samples are very high- dimensional, like images.\n",
    "\n",
    "But what constitutes lots of samples is relative -- relative to the size and depth of the network you’re trying to train, for starters. It isn’t possible to train a convnet to solve a complex problem with just a few tens of samples, but a few hundred can potentially suffice if the model is small and well regularized and the task is simple. Because convnets learn local, translation-invariant features, they’re highly data efficient on perceptual problems. Training a convnet from scratch on a very small image dataset will still yield reasonable results despite a relative lack of data, without the need for any custom feature engineering. You’ll see this in action in this section.\n",
    "\n",
    "What’s more, deep-learning models are by nature highly repurposable: you can take, say, an image-classification or speech-to-text model trained on a large-scale dataset and reuse it on a significantly different problem with only minor changes. Specifically, in the case of computer vision, many pretrained models (usually trained on the ImageNet dataset) are now publicly available for download and can be used to bootstrap powerful vision models out of very little data. That’s what you’ll do in the next section. Let’s start by getting your hands on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part23'></a>\n",
    "### Part 2.3 - Data setup\n",
    "\n",
    "\n",
    "The Dogs vs. Cats dataset that you’ll use isn’t packaged with Keras. It was made available by Kaggle as part of a computer-vision competition in late 2013, back when convnets weren’t mainstream. The data has been downloaded for you from [https://www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data) \n",
    "\n",
    "The pictures are medium-resolution color JPEGs. Unsurprisingly, the dogs-versus-cats Kaggle competition in 2013 was won by entrants who used convnets. The best entries achieved up to 95% accuracy. In this example, you’ll get fairly close to this accuracy (in the next section), even though you’ll train your models on less than 10% of the data that was available to the competitors.\n",
    "\n",
    "A new dataset containing three subsets has been created for you: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set your base dir to your correct local location\n",
    "base_dir = 'cats_and_dogs_small'\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "# Set up directory information\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "\n",
    "print('total training cat images:', len(os.listdir(train_cats_dir))) \n",
    "print('total training dog images:', len(os.listdir(train_dogs_dir))) \n",
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\n",
    "print('total test cat images:', len(os.listdir(test_cats_dir))) \n",
    "print('total test dog images:', len(os.listdir(test_dogs_dir))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you do indeed have 2,000 training images, 1,000 validation images, and 1,000 test images. Each split contains the same number of samples from each class: this is a balanced binary-classification problem, which means classification accuracy will be an appropriate measure of success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part24'></a>\n",
    "### Part 2.4 - Building your network\n",
    "\n",
    "You built a small convnet for MNIST in the previous example, so you should be familiar with such convnets. You’ll reuse the same general structure: the convnet will be a stack of alternated `Conv2D` (with `relu` activation) and `MaxPooling2D` layers.\n",
    "\n",
    "But because you’re dealing with bigger images and a more complex problem, you’ll make your network larger, accordingly: it will have one more `Conv2D + MaxPooling2D` stage. This serves both to augment the capacity of the network and to further reduce the size of the feature maps so they aren’t overly large when you reach the `Flatten` layer. Here, because you start from inputs of size $150 \\times 150$ (a somewhat arbitrary choice), you end up with feature maps of size $7 \\times 7$ just before the Flatten layer.\n",
    "\n",
    "\n",
    "**NOTE:** The depth of the feature maps progressively increases in the network (from 32 to 128), whereas the size of the feature maps decreases (from $148 \\times 148$ to $7 \\times 7$). This is a pattern you’ll see in almost all convnets.\n",
    "\n",
    "Because you’re attacking a binary-classification problem, you’ll end the network with a single unit (a `Dense` layer of size 1) and a `sigmoid` activation. This unit will encode the probability that the network is looking at one class or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the compilation step, you’ll go with the `RMSprop` optimizer. Because you ended the network with a single sigmoid unit, you’ll use `binary crossentropy` as the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "                      metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part25'></a>\n",
    "### Part 2.5 - Data Preprocessing\n",
    "\n",
    "As you know by now, data should be formatted into appropriately preprocessed floating-point tensors before being fed into the network. Currently, the data sits on a drive as JPEG files, so the steps for getting it into the network are roughly as follows:\n",
    "\n",
    "1. Read the picture files.\n",
    "2. Decode the JPEG content to RGB grids of pixels.\n",
    "3. Convert these into floating-point tensors.\n",
    "4. Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values).\n",
    "\n",
    "It may seem a bit daunting, but fortunately Keras has utilities to take care of these steps automatically. Keras has a module with image-processing helper tools, located at `keras.preprocessing.image`. In particular, it contains the class ImageDataGenerator, which lets you quickly set up Python generators that can automatically turn image files on disk into batches of preprocessed tensors. This is what you’ll use here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the output of one of these generators: it yields batches of $150 \\times 150$ RGB images (shape `(20, 150, 150, 3)`) and binary labels (shape `(20,)`). There are 20 samples in each batch (the batch size). Note that the generator yields these batches indefinitely: it loops endlessly over the images in the target folder. For this reason, you need to break the iteration loop at some point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s fit the model to the data using the generator. You do so using the `fit_generator` method, the equivalent of `fit` for data generators like this one. It expects as its first argument a `Python` generator that will yield batches of inputs and targets indefinitely, like this one does. Because the data is being generated endlessly, the Keras model needs to know how many samples to draw from the generator before declaring an epoch over. This is the role of the steps_per_epoch argument: after having drawn `steps_per_epoch` batches from the generator—that is, after having run for `steps_per_epoch` gradient descent steps - the fitting process will go to the next epoch. In this case, batches are 20 samples, so it will take 100 batches until you see your target of 2,000 samples.\n",
    "\n",
    "When using `fit_generator`, you can pass a `validation_data` argument, much as with the `fit` method. It’s important to note that this argument is allowed to be a data generator, but it could also be a tuple of `Numpy` arrays. If you pass a generator as `validation_data`, then this generator is expected to yield batches of validation data endlessly; thus you should also specify the `validation_steps` argument, which tells the process how many batches to draw from the validation generator for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=5, # TODO: should be 30\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)\n",
    "\n",
    "\n",
    "# It’s good practice to always save your models after training.\n",
    "model.save('cats_and_dogs_small_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s plot the loss and accuracy of the model over the training and validation data during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "ax.plot(epochs, acc, 'ro', label='Training acc')\n",
    "ax.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "ax.set_title('Accuracy with data augmentation', fontsize=22)\n",
    "ax.set_xlabel(r'Epochs', fontsize=22)\n",
    "ax.set_ylabel(r'Accuracy', fontsize=22)\n",
    "ax.tick_params(labelsize=22)\n",
    "ax.legend(fontsize=24)\n",
    "\n",
    "figL, axL = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "axL.plot(epochs, loss, 'ro', label='Training loss')\n",
    "axL.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "axL.set_title('Loss with data augmentation', fontsize=22)\n",
    "axL.set_xlabel(r'Epochs', fontsize=22)\n",
    "axL.set_ylabel(r'Loss', fontsize=22)\n",
    "axL.tick_params(labelsize=22)\n",
    "axL.legend(fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots are characteristic of overfitting. The training accuracy increases linearly, whereas the validation accuracy stalls at 70–72%. The validation loss reaches its minimum after only five epochs and then stalls, whereas the training loss keeps decreasing linearly.\n",
    "\n",
    "Because you have relatively few training samples (2,000), overfitting will be your number-one concern. You already know about a number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). We’re now going to work with a new one, specific to computer vision and used almost universally when processing images with deep-learning models: *data augmentation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1'></a>\n",
    "## Part 1 - Data Augmentation\n",
    "\n",
    "Overfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your model would be exposed to every possible aspect of the data distribution at hand: you would never overfit. Data augmentation takes the approach of generating more training data from existing training samples, by *augmenting* the samples via a number of random transformations that yield believable-looking images. The goal is that at training time your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better.\n",
    "\n",
    "In `Keras`, this can be done by configuring a number of random transformations to be performed on the images read by the `ImageDataGenerator` instance. Let’s get started with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few of the options available (for more, see the [`Keras` documentation](https://keras.io/preprocessing/image/)). Let’s quickly go over this code:\n",
    "* `rotation_range` is a value in degrees (0–180), a range within which to randomly rotate pictures.\n",
    "* `width_shift` and `height_shift` are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\n",
    "* `shear_range` is for randomly applying shearing transformations.\n",
    "* `zoom_range` is for randomly zooming inside pictures.\n",
    "* `horizontal_flip` is for randomly flipping half the images horizontally—relevant when there are no assumptions of horizontal asymmetry (for example, real-world pictures).\n",
    "* `fill_mode` is the strategy used for filling in newly created pixels, which can\n",
    "appear after a rotation or a width/height shift. Let’s look at the augmented images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "fnames = [os.path.join(train_dogs_dir, fname) for\n",
    "     fname in os.listdir(train_dogs_dir)]\n",
    "img_path = fnames[3] # Chooses one image to augment\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "# Reads the image and resizes it\n",
    "x = image.img_to_array(img) # Converts it to a Numpy array with shape (150, 150, 3) \n",
    "x = x.reshape((1,) + x.shape) # Reshapes it to (1, 150, 150, 3)\n",
    "i=0\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    plt.figure(i)\n",
    "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 4 == 0:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you train a new network using this data-augmentation configuration, the network will never see the same input twice. But the inputs it sees are still heavily intercorrelated, because they come from a small number of original images—you can’t produce new information, you can only remix existing information. As such, this may not be enough to completely get rid of overfitting. To further fight overfitting, you’ll also add a `Dropout` layer to your model right before the densely connected classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s train the network using data augmentation and dropout.\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# Note that the validation data shouldn’t be augmented!\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=5, # TODO: should be 100\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)\n",
    "\n",
    "model.save('cats_and_dogs_small_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let’s plot the results again. Thanks to data augmentation and dropout, you’re no longer overfitting: the training curves are closely tracking the validation curves. You now reach an accuracy of 82%, a 15% relative improvement over the non-regularized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "ax.plot(epochs, acc, 'ro', label='Training acc')\n",
    "ax.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "ax.set_title('Accuracy with data augmentation', fontsize=22)\n",
    "ax.set_xlabel(r'Epochs', fontsize=22)\n",
    "ax.set_ylabel(r'Accuracy', fontsize=22)\n",
    "ax.tick_params(labelsize=22)\n",
    "ax.legend(fontsize=24)\n",
    "\n",
    "figL, axL = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "axL.plot(epochs, loss, 'ro', label='Training loss')\n",
    "axL.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "axL.set_title('Loss with data augmentation', fontsize=22)\n",
    "axL.set_xlabel(r'Epochs', fontsize=22)\n",
    "axL.set_ylabel(r'Loss', fontsize=22)\n",
    "axL.tick_params(labelsize=22)\n",
    "axL.legend(fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By using regularization techniques even further, and by tuning the network’s parameters (such as the number of filters per convolution layer, or the number of layers in the network), you may be able to get an even better accuracy, likely up to 86% or 87%. But it would prove difficult to go any higher just by training your own convnet from scratch, because you have so little data to work with. As a next step to improve your accuracy on this problem, you’ll have to use a pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "### References and Acknowledgements\n",
    "\n",
    "* Most of this lab is based on the book [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python), Chapter 5 written by the Francois Chollet, the author of Keras. It is a very practical introduction to Deep Learning. It is appropriate for those with some Python knowledge who want to start with machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2'></a>\n",
    "## Part 2 - Comparing Two Networks\n",
    "\n",
    "We will now compare a Fully Connected Network (Multi-Layer Perceptron) and a simple CNN on the task of image classification. We'll use a well known open dataset: CIFAR10. We will be using Keras for our networks.\n",
    "\n",
    "CIFAR10 is a classic dataset released by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton (Machine learning legends). It consists of 60,000 32x32 colour images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. \n",
    "\n",
    "Here are some examples of the images in the dataset:\n",
    "\n",
    "<img style=\"width:500px; height:400px\" src=\"http://clouddatafacts.com/_images/cifar_10.png\">\n",
    "\n",
    "\n",
    "Good news: Keras allows us to easily imports well-known datasets like CIFAR10. We'll import the CIFAR10 dataset with `keras.datasets.cifar10.load_data()`. This will return two tuples of numpy arrays: `(x_train, y_train), (x_test, y_test)`. \n",
    "\n",
    "After that, we will implement an MLP and a CNN to classify the 10 classes of CIFAR10. \n",
    "\n",
    "Keras can build models in two different ways: *Sequential* and *Functional*. \n",
    "\n",
    "The Sequential API is a good starting point, as it allows you to easily create models layer by layer. You used it during the 109 part of the homework. Building a Sequential model just requires to instantiate a `Sequential()` object with `model = Sequential()`, and adding layers after that is easily done with `model.add(layer)`. This API has several limitations, as it does not allow you to easily create bypass connections, share layers between models or have multiple inputs or outputs.\n",
    "\n",
    "Small example of the Sequential API:\n",
    "```\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=1))\n",
    "model.add(Dense(1))\n",
    "```\n",
    "\n",
    "The functional API allows you to create models that have a lot more flexibility as you can easily define models where layers connect to more than just the previous and next layers. You can connect layers to any other layer, add more inputs to your network (even in the middle of it) and concatenating outputs easily. Creating complex networks, such as ResNets, becomes feasible.\n",
    "\n",
    "Small example of the Functional API:\n",
    "```\n",
    "# We define an Input layer\n",
    "inp = Input(shape=(64,64,3))\n",
    "\n",
    "# We instantiate a Dense layer and connect it to the previous layer with (inp)\n",
    "x = Dense(10)(inp)\n",
    "\n",
    "# We repeat the process, connecting here with (x)\n",
    "x= Dense(10)(x)\n",
    "\n",
    "out = Dense(1)(x)\n",
    "\n",
    "# We build the full model\n",
    "model = Model(inputs=in, outputs=out)\n",
    "```\n",
    "\n",
    "We will be using the Functional API for this problem, as this is the main mode that you will be using should you decide to build a serious network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Input, Flatten, Dropout, UpSampling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the `CIFAR10` dataset, one-hot encode the labels and scale the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data() # Load CIFAR10 dataset\n",
    "\n",
    "# Convert to categorical\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Scale the data\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5, figsize=(10,5))\n",
    "row = 0\n",
    "for i in range(10):\n",
    "    if i > 4:\n",
    "        row = 1\n",
    "    ax[row][i%5].imshow(x_train[i])\n",
    "    ax[row][i%5].tick_params(labelbottom=False, labelleft=False)\n",
    "\n",
    "plt.suptitle('Training images')\n",
    "    \n",
    "fig, ax = plt.subplots(2,5, figsize=(10,5))\n",
    "row = 0\n",
    "for i in range(10):\n",
    "    if i > 4:\n",
    "        row = 1\n",
    "    ax[row][i%5].imshow(x_train[i])\n",
    "    ax[row][i%5].tick_params(labelbottom=False, labelleft=False)\n",
    "\n",
    "plt.suptitle('Testing images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The MLP\n",
    "We will use the `Keras` functional API to build a fully-connected network with the following layers:\n",
    "1. Input layer\n",
    "2. Flatten layer (so that we can feed easily to Dense layers afterwards)\n",
    "3. Dense layer, 512 nodes, relu activation\n",
    "3. Dropout layer, 0.2 probability\n",
    "4. Dense layer, 512 nodes, relu activation\n",
    "3. Dropout layer, 0.2 probability\n",
    "5. Dense layer, 256 nodes, relu activation\n",
    "3. Dropout layer, 0.2 probability\n",
    "5. Dense layer, 128 nodes, relu activation\n",
    "3. Dropout layer, 0.2 probability\n",
    "6. Dense layer, 10 nodes, softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP network\n",
    "\n",
    "inp = Input(shape = (32,32,3))\n",
    "x = Flatten()(inp)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll compile the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs = 10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All trained!  That took a while on the CPU, but we made it through.  Now we need to evaluate the peformance on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn.\n",
    "\n",
    "Build a CNN with the following layers:\n",
    "1. Conv2D, 32 3x3 filters, (1,1) strides, padding=same, activation=relu, use_bias=True\n",
    "2. Conv2D, 32 3x3 filters, (1,1) strides, padding=same, activation=relu, use_bias=True\n",
    "3. Maxpool, strides (2,2), pool_size (2,2)\n",
    "1. Conv2D, 64 3x3 filters, (1,1) strides, padding=same, activation=relu, use_bias=True\n",
    "2. Conv2D, 64 3x3 filters, (1,1) strides, padding=same, activation=relu, use_bias=True\n",
    "3. Maxpool, strides (2,2), pool_size (2,2)\n",
    "1. Conv2D, 128 3x3 filters, (1,1) strides, padding=same, activation=relu, use_bias=True\n",
    "2. Conv2D, 128 3x3 filters, (1,1) strides, padding=same, activation=relu, use_bias=True\n",
    "3. Maxpool, strides (2,2), pool_size (2,2)\n",
    "5. Flatten layer\n",
    "6. Dropout layer, 0.2 probability\n",
    "7. Dense layer, 512 nodes, relu activation\n",
    "8. Dropout, 0.5 probability\n",
    "9. Dense layer, 10 nodes, softmax activation\n",
    "\n",
    "\n",
    "* Use the `Keras` functional API.\n",
    "* Train the CNN network\n",
    "* Evaluate the CNN model on the test data set\n",
    "* Discuss the differences between the MLP and the CNN\n",
    "\n",
    "**Remarks:**\n",
    "\n",
    "Use the same loss function, optimizer, number of epochs, batch size, and validation split size as you did when you trained the MLP.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "Here are the definitions of generic Conv2D layers, MaxPooling2D layers, Dense, Flatten and Dropout layers:\n",
    "\n",
    "```python\n",
    "x = Conv2D(32, (3,3), strides=(1, 1), padding='same', activation='relu', use_bias=True)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
