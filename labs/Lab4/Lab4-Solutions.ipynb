{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4:  Regularization\n",
    "\n",
    "## Saturday, November 3rd 2018\n",
    "\n",
    "### David Sondak and Pavlos Protopapas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "Lecture 3 introduced several types of regularization.  In today's lab, you will become more familiar with those regularization techniques and actually apply them to a problem.  The types of regularization that you will explore today are:\n",
    "* Penalization\n",
    "* Early stopping\n",
    "* Dropout\n",
    "There are many other types of regularization (as mentioned in lecture).  The three regularization techniques that you will explore today are very popular and used frequently in real applications.\n",
    "\n",
    "We'll begin the story by building a neural network to learn a function from some noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warming Up\n",
    "Today we'll try to fit the function $$f\\left(x\\right) = x\\sin\\left(x\\right).$$\n",
    "\n",
    "Using `keras`, build a fully-connected neural network to fit $f\\left(x\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras import layers\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll generate some synthetic data with some synthetic noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100 # set the number of samples to take for each toy dataset\n",
    "test_size = 0.2 # set the proportion of toy data to hold out for testing\n",
    "random_seed = 1 # set the random seed to make the experiment reproducible \n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# define a function\n",
    "f = lambda x: x * np.sin(x)\n",
    "\n",
    "# Generate the truth function (without any noise)\n",
    "X_true = np.linspace(0.0, 5.0, n_samples)\n",
    "Y_true = f(X_true)\n",
    "\n",
    "# Now sample the true function at some points\n",
    "X = np.random.permutation(X_true) # choose some points from the function - this is our toy dataset \n",
    "Y = f(X)\n",
    "\n",
    "Y = Y + np.random.normal(0.0, 1.0, len(Y)) # Add some noise from a random normal distribution\n",
    "\n",
    "# create training and testing data from this set of points\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build a network.  We choose $5$ hidden layers and $100$ nodes per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 5\n",
    "N = 100\n",
    "\n",
    "input_dim = 1\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(N, input_dim=input_dim, activation='relu'))\n",
    "\n",
    "# Create hidden layers\n",
    "for h in range(num_layers):\n",
    "    model.add(layers.Dense(N, activation='relu'))\n",
    "    \n",
    "model.add(layers.Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll fit the model.  \n",
    "\n",
    "Notice that we're specifying a *validation set*.  What this means is that `keras` will further split the training set into a training part and a validation part.  The neural network will only be trained on the *training* set.  Meanwhile, `keras` will report performance metrics on the *validation* set so we can get a sense of how well the model has been trained.  We will be using the validation set quite a bit in this lab.\n",
    "\n",
    "Remember, we don't want to use the test set for anything relating to the training of our models.  By withholding the validation set, we can assess the model performance on the validation set.  Later, we can see how the model performs on data it has never seen before by using in on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "no_reg = model.fit(X_train, Y_train, epochs=2500, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is trained.  Let's see what the solution looks like.\n",
    "\n",
    "We will also get the validation data set that `keras` created for us.  By plotting the validation data, we will gain some insight into how well the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set\n",
    "X_val = no_reg.validation_data[0]\n",
    "Y_val = no_reg.validation_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use our model to predict in the range we want\n",
    "X_range = np.linspace(0.0, 5, 1000)\n",
    "y_pred = model.predict(X_range)\n",
    "\n",
    "# Now plot everything\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14,8))\n",
    "ax.plot(X_true, Y_true, ls='-.', lw=4, label='True function')\n",
    "ax.plot(X_train, Y_train, '.', label='Training data')\n",
    "ax.plot(X_val, Y_val, '.', label='Validation data')\n",
    "ax.plot(X_range, y_pred, lw=4, label='Prediction')\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.legend(loc=1, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction looks pretty bad.  The neural network model is trying to go through all the training points.  This is a classic case of overfitting.  The solution has a lot of oscillations and it rarely fits the validation data.  It may be a good idea to use some kind of regularization.\n",
    "\n",
    "Let's begin with some penalization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalization\n",
    "As mentioned in lecture, the loss function can be augmented by an additional term called the penalization term.  Mathematicall, the goal is to find the set of weights $W$ that minimize the functional $$J_{R}\\left(W; X, y\\right) = J\\left(W; X, y\\right) + \\alpha\\Omega\\left(W\\right)$$\n",
    "where $\\alpha$ is called the regularization (or penalization) parameter.  In this lab, $\\displaystyle J\\left(W; X, y\\right)$ is the MSE loss function.\n",
    "\n",
    "Next, we consider the effect of two different forms for the penalization term: $L_{1}$ and $L_{2}$ penalization.  For reference, \n",
    "$$\\Omega_{L_{1}} = \\frac{1}{2}\\left\\|W\\right\\|_{1}$$\n",
    "and \n",
    "$$\\Omega_{L_{2}} = \\frac{1}{2}\\|W\\|^{2}_{2}.$$\n",
    "\n",
    "Note that the biases are not being penalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Fit the same network as above ($5$ hidden layers, $100$ nodes per layer, linear output layer), but this time use $L_{2}$ and $L_{1}$ regularization.\n",
    "\n",
    "**Deliverables:**\n",
    "* Make two figures, one on top of the other.\n",
    "* The first figure should contain the following:\n",
    "  - True solution\n",
    "  - Training data\n",
    "  - Validation data\n",
    "  - Neural network prediction without regularization\n",
    "  - Neural network prediction with $L_{2}$ regularization\n",
    "* The second figure should contain the following:\n",
    "  - True solution\n",
    "  - Training data\n",
    "  - Validation data\n",
    "  - Neural network prediction without regularization\n",
    "  - Neural network prediction with $L_{1}$ regularization\n",
    "* **Make sure everything is clearly labeled!**\n",
    "* Discuss the results.\n",
    "\n",
    "**Hints:**\n",
    "* Use `kernel_regularizer=regularizers.l2(alpha)` after the `activation` argument in each of your layers.\n",
    "* Choose a value for `alpha` that you think works well.  You may need to play around with this a little bit.\n",
    "* See the `Keras` documentation on regularization:  [Usage of regularizers](https://keras.io/regularizers/)\n",
    "* Here's some pseudo-code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from keras import regularizers\n",
    "\n",
    "num_layers = 5\n",
    "N = 100\n",
    "alpha = \n",
    "\n",
    "input_dim = 1\n",
    "\n",
    "### Create network\n",
    "model_L2 = \n",
    "model_L2.add()\n",
    "\n",
    "\n",
    "### Compile network\n",
    "model_L2.compile()\n",
    "\n",
    "### Fit model\n",
    "L2_reg = model_L2.fit()\n",
    "\n",
    "### Extract validation data\n",
    "X_val_L2 = \n",
    "Y_val_L2 = \n",
    "\n",
    "### REPEAT FOR L1\n",
    "###\n",
    "###\n",
    "###\n",
    "\n",
    "# PLOT\n",
    "fig, ax = plt.subplots(2, 1, figsize=(20,14), sharex=True)\n",
    "\n",
    "ax[0].plot() # Top plots\n",
    "### ...\n",
    "\n",
    "ax[0].set_ylabel(r'$Y$', fontsize=20)\n",
    "ax[0].tick_params(labelsize=20)\n",
    "ax[0].legend(loc=1, fontsize=20)\n",
    "\n",
    "\n",
    "ax[1].plot() # Bottom plots\n",
    "### ...\n",
    "ax[1].set_xlabel(r'$Y$', fontsize=20)\n",
    "ax[1].set_ylabel(r'$Y$', fontsize=20)\n",
    "ax[1].tick_params(labelsize=20)\n",
    "ax[1].legend(loc=1, fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L_{2}$ Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "num_layers = 5\n",
    "N = 100\n",
    "alpha = 0.005\n",
    "\n",
    "input_dim = 1\n",
    "\n",
    "model_L2 = models.Sequential()\n",
    "\n",
    "model_L2.add(layers.Dense(N, input_dim=input_dim, kernel_initializer='normal', activation='relu', \n",
    "                          kernel_regularizer=regularizers.l2(alpha)))\n",
    "\n",
    "for h in range(num_layers):\n",
    "    model_L2.add(layers.Dense(N, activation='relu', kernel_regularizer=regularizers.l2(alpha)))\n",
    "    \n",
    "model_L2.add(layers.Dense(1, activation='linear', kernel_regularizer=regularizers.l2(alpha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_L2.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L2_reg = model_L2.fit(X_train, Y_train, epochs=2500, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set\n",
    "X_val_L2 = L2_reg.validation_data[0]\n",
    "Y_val_L2 = L2_reg.validation_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L_{1}$ Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.005\n",
    "\n",
    "input_dim = 1\n",
    "\n",
    "model_L1 = models.Sequential()\n",
    "\n",
    "model_L1.add(layers.Dense(N, input_dim=input_dim, kernel_initializer='normal', activation='relu', \n",
    "                          kernel_regularizer=regularizers.l1(alpha)))\n",
    "\n",
    "for h in range(num_layers):\n",
    "    model_L1.add(layers.Dense(N, activation='relu', kernel_regularizer=regularizers.l1(alpha)))\n",
    "    \n",
    "model_L1.add(layers.Dense(1, activation='linear', kernel_regularizer=regularizers.l1(alpha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_L1.compile(loss='mean_squared_error', optimizer='adam')\n",
    "L1_reg = model_L1.fit(X_train, Y_train, epochs=2500, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_L1 = L1_reg.validation_data[0]\n",
    "Y_val_L1 = L1_reg.validation_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our model to predict in the range we want\n",
    "X_range = np.linspace(0.0, 5, 1000)\n",
    "y_pred_L2 = model_L2.predict(X_range)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(20,14), sharex=True)\n",
    "\n",
    "ax[0].plot(X_true, Y_true, color='k', ls='-.', lw=4, label='True function')\n",
    "ax[0].plot(X_train, Y_train, '.', label='Training data')\n",
    "ax[0].plot(X_test, Y_test, ls='', marker='^',  ms=12, label='Test data')\n",
    "ax[0].plot(X_range, y_pred, lw=4, color='r', label='Prediction')\n",
    "ax[0].plot(X_range, y_pred_L2, lw=4, ls='--', color='g', label=r'$L_{2}$ Prediction')\n",
    "\n",
    "ax[0].set_ylabel(r'$Y$', fontsize=20)\n",
    "ax[0].tick_params(labelsize=20)\n",
    "\n",
    "ax[0].legend(loc=1, fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "y_pred_L1 = model_L1.predict(X_range)\n",
    "\n",
    "ax[1].plot(X_true, Y_true, color='k', ls='-.', lw=4, label='True function')\n",
    "ax[1].plot(X_train, Y_train, '.', label='Training data')\n",
    "ax[1].plot(X_test, Y_test, ls='', marker='^',  ms=12, label='Test data')\n",
    "ax[1].plot(X_range, y_pred, lw=4, color='r', label='Prediction')\n",
    "ax[1].plot(X_range, y_pred_L1, lw=4, ls='--', color='g', label=r'$L_{1}$ Prediction')\n",
    "\n",
    "ax[1].set_xlabel(r'$X$', fontsize=20)\n",
    "ax[1].set_ylabel(r'$Y$', fontsize=20)\n",
    "ax[1].tick_params(labelsize=20)\n",
    "\n",
    "ax[1].legend(loc=1, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping\n",
    "The results without any regularization do not look right.  $L_{2}$ and $L_{1}$ regularizaton helped somewhat, but the results still aren't convincing.\n",
    "\n",
    "We can gain some more insight by plotting the loss functions from the training and validation set.  Let's use a `log-log` scale to enhance any discrepancies between the two curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a reminder.\n",
    "\n",
    "Remember that the `fit` method can store the history of the model.  For the unregularized model we stored all the history in the name `no_reg`.  Let's see what attributes are in that object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(no_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of stuff; most of it we're not interested in.  However, at the very end of the list, we see some useful keys.  Let's access some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(no_reg.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like `history` is a dictionary.  Let's take a look at it's keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_reg.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very cool.  There is a `validation` and `training` loss.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the one we'll want to use right now, but we can look at the other attributes too just to get a feel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(no_reg.validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(no_reg.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_reg.params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_reg.params['batch_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that was fun and informative.  But what we're really after is the loss data as a function of epoch number.  Here we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = no_reg.history['loss']\n",
    "L_val = no_reg.history['val_loss']\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "ax.plot(L, label='Training Loss')\n",
    "ax.plot(L_val, label='Validation Loss')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_xlabel(r'Epochs', fontsize=20)\n",
    "ax.set_ylabel(r'Loss', fontsize=20)\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.legend(loc=1, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow.  That is striking.\n",
    "\n",
    "We used $2500$ epochs, but the validation loss begins to rise at around $50$ epochs and becomes larger than the training loss at around $70$ epochs.  After that, we're basically overfitting.\n",
    "\n",
    "Notice that the training loss keeps decreasing.  We're fitting the training data better and better all the time.  The validation loss is getting larger and larger meaning that we're losing generalizability.\n",
    "\n",
    "We can use this new information to our advantage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Part 1\n",
    "Train a network without any penalization, but this time stop after $20$ epochs.\n",
    "\n",
    "### Part 2\n",
    "Train a network without any penalization, but this time stop at the \"optimal\" number of epochs (based on the crossing of the loss curves).\n",
    "\n",
    "**Deliverables**\n",
    "* Plot the following on a single figure:\n",
    "  - The true solution\n",
    "  - The model prediction without any regularization (after $2500$ epochs)\n",
    "  - The model prediction without any regularization using $20$ epochs\n",
    "  - The model prediction without any regularization using the optimal number of epochs\n",
    "* You may also want to include the training and validation data on the same plot.  Be careful that the plot doesn't become too cluttered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 5\n",
    "N = 100\n",
    "\n",
    "input_dim = 1\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(N, input_dim=input_dim, activation='relu'))\n",
    "\n",
    "# Create hidden layers\n",
    "for h in range(num_layers):\n",
    "    model.add(layers.Dense(N, activation='relu'))\n",
    "    \n",
    "model.add(layers.Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_reg_20 = model.fit(X_train, Y_train, epochs=20, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_range = np.linspace(0.0, 5, 1000)\n",
    "y_pred_20 = model.predict(X_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 5\n",
    "N = 100\n",
    "\n",
    "input_dim = 1\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(N, input_dim=input_dim, activation='relu'))\n",
    "\n",
    "# Create hidden layers\n",
    "for h in range(num_layers):\n",
    "    model.add(layers.Dense(N, activation='relu'))\n",
    "    \n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "no_reg_75 = model.fit(X_train, Y_train, epochs=75, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_75 = model.predict(X_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,8))\n",
    "\n",
    "ax.plot(X_true, Y_true, color='k', ls='-.', lw=4, label='True function')\n",
    "ax.plot(X_train, Y_train, '.', label='Training data')\n",
    "ax.plot(X_test, Y_test, ls='', marker='^',  ms=12, label='Test data')\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label='Prediction')\n",
    "ax.plot(X_range, y_pred_20, lw=4, ls='--', color='g', label=r'$20$ Epochs')\n",
    "ax.plot(X_range, y_pred_75, lw=4, ls=':', color='m', label=r'$75$ Epochs')\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.legend(loc=3, ncol=2, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to do things more systematically.\n",
    "\n",
    "How do you think early stopping should be implemented?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do early stopping in `Keras`, you specify the `EarlyStopping` [*callback*](https://keras.io/callbacks/).  From the documentation:\n",
    "> A callback is a set of functions to be applied at given stages of the training procedure.\n",
    "\n",
    "Callbacks can be used to view internal states and statistic of the model during training.\n",
    "\n",
    "Right now, we'll use one to monitor the validation loss function.  When the validation loss starts to go up, the training process will stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Syntax\n",
    "To specify a callback, you just pass a `callbacks` list into the model `fit()` method, like this:\n",
    "```python\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=2500, batch_size=64, validation_split=0.2, \n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Fit the model using the early stopping technique.  Try different values for `patience` to see which one gives you the lowest validation loss.\n",
    "\n",
    "How many epochs are needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "num_layers = 5\n",
    "N = 100\n",
    "\n",
    "input_dim = 1\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(N, input_dim=input_dim, activation='relu'))\n",
    "\n",
    "# Create hidden layers\n",
    "for h in range(num_layers):\n",
    "    model.add(layers.Dense(N, activation='relu'))\n",
    "    \n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fit model\n",
    "no_reg_ES = model.fit(X_train, Y_train, epochs=2500, batch_size=64, validation_split=0.2, \n",
    "                     callbacks=[EarlyStopping(monitor='val_loss', patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ES = model.predict(X_range)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,8))\n",
    "\n",
    "ax.plot(X_true, Y_true, color='k', ls='-.', lw=4, label='True function')\n",
    "ax.plot(X_train, Y_train, '.', label='Training data')\n",
    "ax.plot(X_test, Y_test, ls='', marker='^',  ms=12, label='Test data')\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label='Prediction')\n",
    "ax.plot(X_range, y_pred_ES, lw=4, ls=':', color='b', label=r'Early Stopping')\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.legend(loc=3, ncol=2, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution should looks pretty good.  Of course, we had to play with the `patience` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "Remind them what it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "num_layers = 5\n",
    "N = 100\n",
    "\n",
    "input_dim = 1\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(N, input_dim=input_dim, activation='relu'))\n",
    "#model.add(layers.Dropout(1.0))\n",
    "\n",
    "# Create hidden layers\n",
    "for h in range(num_layers):\n",
    "    model.add(layers.Dense(N, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fit model\n",
    "no_reg_dropout = model.fit(X_train, Y_train, epochs=250, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dropout = model.predict(X_range)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,8))\n",
    "\n",
    "ax.plot(X_true, Y_true, color='k', ls='-.', lw=4, label='True function')\n",
    "ax.plot(X_train, Y_train, '.', label='Training data')\n",
    "ax.plot(X_test, Y_test, ls='', marker='^',  ms=12, label='Test data')\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label='Prediction')\n",
    "ax.plot(X_range, y_pred_ES, lw=4, ls='--', color='b', label=r'Early Stopping')\n",
    "ax.plot(X_range, y_pred_dropout, lw=4, ls=':', color='m', label=r'Dropout')\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.legend(loc=3, ncol=2, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = no_reg_dropout.history['loss']\n",
    "L_val = no_reg_dropout.history['val_loss']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "ax.plot(L, label='Training Loss')\n",
    "ax.plot(L_val, label='Validation Loss')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_xlabel(r'Epochs', fontsize=20)\n",
    "ax.set_ylabel(r'Loss', fontsize=20)\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.legend(loc=1, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: `GPU` on `AWS`\n",
    "Have them redo the calculations on `AWS` using a `GPU` instance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
